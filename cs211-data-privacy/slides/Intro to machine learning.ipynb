{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Some useful utilities\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "def z_clip(xs, b):\n",
    "    return [min(x, b) for x in xs]\n",
    "\n",
    "def g_clip(v):\n",
    "    n = np.linalg.norm(v, ord=2)\n",
    "    if n > 1:\n",
    "        return v / n\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset files you'll need are available here:\n",
    "\n",
    "- [`adult_processed_x`](https://github.com/jnear/cs295-data-privacy/blob/master/slides/adult_processed_x.npy)\n",
    "- [`adult_processed_y`](https://github.com/jnear/cs295-data-privacy/blob/master/slides/adult_processed_y.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally split our data into a \"training set\" and a \"test set.\" We train the model on the training set, and test its accuracy on the test set. The goal is to avoid \"overfitting\" - a model that simply memorizes the training data, and can't generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9044,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll mainly use logistic regression, but the techniques we'll see apply to a number of different machine learning tasks. Scikit-learn has logistic regression built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X_train,y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the test set to test our new model\n",
    "model.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84464838567005751"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio of correct answers to total\n",
    "np.sum(model.predict(X_test) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our own Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does Scikit-learn actually do? We'll look at a simple but effective algorithm for training - gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(theta, xi):\n",
    "    label = np.sign(xi @ theta)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can take an initial guess at theta - a vector of all zeros.\n",
    "# It won't be very accurate.\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "predict(theta, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the loss function to measure the effectiveness of our current theta\n",
    "loss(np.reshape(model.coef_, (104, 1)), X_train[0], y_train[0])\n",
    "loss(theta, X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([loss(theta, X_train[i], y_train[i]) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates in which direction the loss function is increasing fastest\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.51351237,\n",
       "        0.        ,  0.        ,  0.        , -0.51351237,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.51351237,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.51351237,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.51351237,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.51351237,\n",
       "        0.        , -0.51351237,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.51351237,  0.        ,  0.        , -0.25675619, -0.06225269,\n",
       "       -0.22466166,  0.        ,  0.        , -0.18154478])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we take a step in the *opposite* direction from the gradient (by negating it), we should \n",
    "# move theta in a direction that makes the loss *lower*\n",
    "# This is one step of gradient descent - in each step, we're trying to \"descend\" the gradient\n",
    "# In this example, we're taking the gradient on just a single training example (the first one)\n",
    "theta = theta - gradient(theta, X_train[0], y_train[0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now our model is better!\n",
    "predict(theta, X_test[0]) == y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's measure the accuracy of our \"zeros theta\" - it's not good\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "[predict(theta, xi) for xi in X_test] == y_test\n",
    "np.sum([predict(theta, xi) for xi in X_test] == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now take one step of gradient descent and try again\n",
    "theta = theta - gradient(theta, X_train[0], y_train[0])\n",
    "[predict(theta, xi) for xi in X_test] == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9044,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.75851393188854488"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The new model labels 75% of the test set correctly!\n",
    "display(y_test.shape)\n",
    "np.sum([predict(theta, xi) for xi in X_test] == y_test) / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75851393188854488"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our measure of accuracy is just % correct of the test set\n",
    "def accuracy(theta, X, y):\n",
    "    return np.sum([predict(theta, xi) for xi in X] == y) / y.shape[0]\n",
    "\n",
    "accuracy(theta, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A step of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic gradient descent algorithm takes the *average* gradient over all of the examples in the training set, and steps in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75851393188854488"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "theta1 = theta - np.mean([gradient(theta, xi, yi) for xi, yi in zip(X_train, y_train)], axis=0)\n",
    "accuracy(theta1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the average gradient a lot, so we define a function for it\n",
    "def avg_grad(theta, X, y):\n",
    "    return np.mean([gradient(theta, xi, yi) for xi, yi in zip(X, y)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55507606543433174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.52743894635227084"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.50711714383727191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4916876964348964"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.47967792569340956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.47011075871430719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.46232417347066274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45586210682683342"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45040486337426139"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.44572431306271937"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.77874834144183991"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the basic gradient descent algorithm, run for 10 steps (10 iterations)\n",
    "# At each step, we print out the loss. Notice that it decreases with each step.\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "for i in range(10):\n",
    "    theta = theta - avg_grad(theta, X_train, y_train)\n",
    "    display(np.mean([loss(theta, X_train[i], y_train[i]) for i in range(10000)]))\n",
    "\n",
    "accuracy(theta, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.63933476e-02,  -2.62737908e-02,  -3.76703876e-01,\n",
       "         5.75414219e-02,  -6.45755008e-02,  -2.73486282e-02,\n",
       "        -1.30672661e-03,  -5.91543132e-02,  -7.64880798e-02,\n",
       "        -2.62061378e-02,  -1.57561761e-02,  -2.86662073e-02,\n",
       "        -4.72301785e-02,  -3.76260473e-02,  -9.90166218e-03,\n",
       "        -1.97245917e-02,   1.55177596e-01,   4.49929106e-02,\n",
       "        -3.29796604e-01,   1.19518106e-01,  -4.89454322e-03,\n",
       "         6.88790663e-02,  -1.55396891e-01,  -1.80599005e-01,\n",
       "         1.54260151e-03,   3.89119966e-01,  -1.97894731e-02,\n",
       "        -5.15748994e-01,  -5.58626968e-02,  -4.09361505e-02,\n",
       "        -1.28065226e-01,  -1.62203385e-04,  -1.08119029e-01,\n",
       "         1.88498244e-01,  -6.47481586e-02,  -8.66006277e-02,\n",
       "        -9.99400278e-02,  -2.05565204e-01,  -1.11388992e-02,\n",
       "         1.61706332e-01,  -3.59862313e-03,  -1.50012146e-02,\n",
       "         1.68869178e-03,  -5.12278071e-02,   3.20211424e-01,\n",
       "        -2.99730645e-01,  -6.21807885e-02,  -2.79987375e-01,\n",
       "        -1.81265739e-01,   8.06793703e-02,  -1.62147320e-02,\n",
       "        -2.28589562e-02,  -1.37829430e-01,  -1.60678844e-02,\n",
       "        -2.29302751e-01,  -3.66995428e-01,  -5.52783255e-02,\n",
       "         1.71465403e-04,   1.01657252e-03,  -2.30554477e-03,\n",
       "        -4.49929315e-03,  -3.16006935e-03,  -5.97944101e-03,\n",
       "        -2.17232333e-03,  -6.49185240e-03,   2.25224908e-03,\n",
       "         1.03307811e-03,  -2.97354842e-03,   4.61969711e-06,\n",
       "        -4.47281179e-03,  -2.64817559e-03,  -6.27231883e-05,\n",
       "        -8.45045871e-04,  -5.36070827e-04,   9.85204107e-05,\n",
       "         8.18802416e-04,   2.81426612e-04,  -1.10188120e-04,\n",
       "        -3.16311117e-04,  -4.14059871e-03,  -2.56618731e-04,\n",
       "        -1.23603629e-03,  -5.52492747e-02,  -2.68278922e-03,\n",
       "        -1.13850454e-03,  -2.58782610e-03,  -4.19239246e-03,\n",
       "        -2.31953654e-03,  -2.62620550e-03,  -8.63337753e-03,\n",
       "        -1.04120760e-03,  -4.43136447e-03,   2.32630683e-04,\n",
       "        -1.21364005e-03,  -1.63560801e-03,  -2.93003921e-01,\n",
       "        -5.03114545e-03,  -1.89672372e-04,  -7.59047983e-02,\n",
       "        -5.58277360e-02,  -6.33588757e-02,   5.91506903e-02,\n",
       "         4.41760150e-02,  -8.32223096e-02])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent with privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is gradient descent with a *learning rate* \"eta\"\n",
    "def gradient_descent():\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    eta = 1.0\n",
    "\n",
    "    for i in range(10):\n",
    "        theta = theta - eta * avg_grad(theta, X_train, y_train)\n",
    "\n",
    "    display(accuracy(theta, X_test, y_test))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77874834144183991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  1.63933476e-02,  -2.62737908e-02,  -3.76703876e-01,\n",
       "         5.75414219e-02,  -6.45755008e-02,  -2.73486282e-02,\n",
       "        -1.30672661e-03,  -5.91543132e-02,  -7.64880798e-02,\n",
       "        -2.62061378e-02,  -1.57561761e-02,  -2.86662073e-02,\n",
       "        -4.72301785e-02,  -3.76260473e-02,  -9.90166218e-03,\n",
       "        -1.97245917e-02,   1.55177596e-01,   4.49929106e-02,\n",
       "        -3.29796604e-01,   1.19518106e-01,  -4.89454322e-03,\n",
       "         6.88790663e-02,  -1.55396891e-01,  -1.80599005e-01,\n",
       "         1.54260151e-03,   3.89119966e-01,  -1.97894731e-02,\n",
       "        -5.15748994e-01,  -5.58626968e-02,  -4.09361505e-02,\n",
       "        -1.28065226e-01,  -1.62203385e-04,  -1.08119029e-01,\n",
       "         1.88498244e-01,  -6.47481586e-02,  -8.66006277e-02,\n",
       "        -9.99400278e-02,  -2.05565204e-01,  -1.11388992e-02,\n",
       "         1.61706332e-01,  -3.59862313e-03,  -1.50012146e-02,\n",
       "         1.68869178e-03,  -5.12278071e-02,   3.20211424e-01,\n",
       "        -2.99730645e-01,  -6.21807885e-02,  -2.79987375e-01,\n",
       "        -1.81265739e-01,   8.06793703e-02,  -1.62147320e-02,\n",
       "        -2.28589562e-02,  -1.37829430e-01,  -1.60678844e-02,\n",
       "        -2.29302751e-01,  -3.66995428e-01,  -5.52783255e-02,\n",
       "         1.71465403e-04,   1.01657252e-03,  -2.30554477e-03,\n",
       "        -4.49929315e-03,  -3.16006935e-03,  -5.97944101e-03,\n",
       "        -2.17232333e-03,  -6.49185240e-03,   2.25224908e-03,\n",
       "         1.03307811e-03,  -2.97354842e-03,   4.61969711e-06,\n",
       "        -4.47281179e-03,  -2.64817559e-03,  -6.27231883e-05,\n",
       "        -8.45045871e-04,  -5.36070827e-04,   9.85204107e-05,\n",
       "         8.18802416e-04,   2.81426612e-04,  -1.10188120e-04,\n",
       "        -3.16311117e-04,  -4.14059871e-03,  -2.56618731e-04,\n",
       "        -1.23603629e-03,  -5.52492747e-02,  -2.68278922e-03,\n",
       "        -1.13850454e-03,  -2.58782610e-03,  -4.19239246e-03,\n",
       "        -2.31953654e-03,  -2.62620550e-03,  -8.63337753e-03,\n",
       "        -1.04120760e-03,  -4.43136447e-03,   2.32630683e-04,\n",
       "        -1.21364005e-03,  -1.63560801e-03,  -2.93003921e-01,\n",
       "        -5.03114545e-03,  -1.89672372e-04,  -7.59047983e-02,\n",
       "        -5.58277360e-02,  -6.33588757e-02,   5.91506903e-02,\n",
       "         4.41760150e-02,  -8.32223096e-02])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a differentially private version of gradient descent:\n",
    "# At each iteration, we use the Gaussian mechanism to add enough noise to the gradient to\n",
    "# make it differentially private. By post-processing, the update we perform afterwards satisfies\n",
    "# differential privacy.\n",
    "# We're assuming the sensitivity is 1 - but it's not. We'll fix that later.\n",
    "def dp_gradient_descent(epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    eta = 1.0\n",
    "\n",
    "    for i in range(10):\n",
    "        theta = theta - eta * gaussian_mech_vec(avg_grad(theta, X_train, y_train), 1, epsilon, delta)\n",
    "\n",
    "    return accuracy(theta, X_test, y_test)\n",
    "    #return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.10000000000000001, 0.37394957983193278),\n",
       " (11.200000000000001, 0.67138434321096863),\n",
       " (22.300000000000004, 0.75077399380804954),\n",
       " (33.400000000000006, 0.74104378593542686),\n",
       " (44.500000000000007, 0.73452012383900933),\n",
       " (55.600000000000009, 0.7568553737284387),\n",
       " (66.700000000000003, 0.76448474126492705),\n",
       " (77.800000000000011, 0.77841662980981863),\n",
       " (88.900000000000006, 0.78814683768244143),\n",
       " (100.0, 0.7816231755860239)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116e05128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The accuracy of the model degrades as we add more noise\n",
    "# This is because the training algorithm can go the wrong direction because of the noise.\n",
    "xs = np.linspace(0.1, 100.0, 10)\n",
    "ys = [dp_gradient_descent(x, 1e-5) for x in xs]\n",
    "plt.plot(xs,ys)\n",
    "display(list(zip(xs,ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can bound the sensitivity of the gradient by clipping\n",
    "# A common strategy is to clip the gradient for *each example* so that it has L2 norm of 1\n",
    "# Then the sensitivity of a single-example gradient is 1\n",
    "\n",
    "# L2 sensitivity: 1 by clipping\n",
    "def clip_gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return g_clip(- (yi*xi) / (1+np.exp(exponent)))\n",
    "\n",
    "# When we take the average over all examples, the sensitivity is 1/n, where n is the size of the training set.\n",
    "# This is just like the averages we've seen in database-style examples: only one example will change\n",
    "# between neighboring datasets, so the sensitivity of the sum of examples is just 1.\n",
    "\n",
    "# L2 sensitivity: 1/X.shape[0] by averaging\n",
    "def avg_clip_grad(theta, X, y):\n",
    "    return np.mean([clip_gradient(theta, xi, yi) for xi, yi in zip(X, y)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use a sensitivity of 1/X_train.shape[0] to get differential privacy for each iteration\n",
    "# This is a huge win - sensitivity is very small when you have a large dataset\n",
    "# As you increase the amount of data you're training on, the sensitivity gets better\n",
    "def dp_gradient_descent(epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    eta = 1.0\n",
    "\n",
    "    for i in range(10):\n",
    "        theta = theta - eta * gaussian_mech_vec(avg_clip_grad(theta, X_train, y_train), 1/X_train.shape[0], epsilon, delta)\n",
    "\n",
    "    return accuracy(theta, X_test, y_test)\n",
    "    #return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.65413533834586468,\n",
       " 0.51658558160106149,\n",
       " 0.47202565236620964,\n",
       " 0.37914639540026535,\n",
       " 0.66298098186643073]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dp_gradient_descent(0.001, 1e-5) for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's missing? Composition! Each step of our algorithm above is differentially private, but we haven't\n",
    "# bounded the *total* privacy cost of the algorithm. There are lots of ways to do this: advanced composition,\n",
    "# Renyi differential privacy, zCDP, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
