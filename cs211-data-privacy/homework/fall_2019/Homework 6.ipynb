{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS295B F19: Homework 6\n",
    "## Differentially Private Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Before you start, download the example dataset and ensure that all cells in this notebook execute without error. If you have trouble getting the notebook to run, please post a question on Piazza.\n",
    "\n",
    "To ensure that the notebook runs, I've defined a function `your_code_here()` that simply returns the number `1`. Whenever you see a call to this function, you should replace it with code you have written. Please make sure all cells of your notebook run without error before submitting the assignment. If you have not completed all the questions, leave calls to `your_code_here()` in place or insert dummy values so that the cell does not throw an error when it runs.\n",
    "\n",
    "To help you arrive at the correct solution, I have left the value computed by my solution in the uploaded version of this notebook. You can refer to these example results by viewing the notebook on Github. If you re-run the cell after downloading the notebook, the results will disappear (because the notebook no longer contains the code that generated them). Your solutions should produce results similar to the ones in the uploaded notebook.\n",
    "\n",
    "When answering non-code questions, feel free to use a comment, or put the cell in Markdown mode and use Markdown.\n",
    "\n",
    "The assignment is due by 5:00pm on Friday, November 8. When you have finished your assignment, submit it via Gradescope under the assignment \"Homework 6.\" For questions on grading and submitting assignments, refer to the course webpage or email the instructor.\n",
    "\n",
    "The dataset files you'll need are available here:\n",
    "\n",
    "- [`adult_processed_x`](https://github.com/jnear/cs295-data-privacy/blob/master/slides/adult_processed_x.npy)\n",
    "- [`adult_processed_y`](https://github.com/jnear/cs295-data-privacy/blob/master/slides/adult_processed_y.npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Statement\n",
    "\n",
    "In the cell below, write your collaboration statement. This statement should describe all collaborations, even high-level ones (e.g. \"I discussed my general approach for answering question 3 with Josh\"). High-level collaborations of this kind are allowed as long as they are described; copying of answers or code is not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your collaboration statement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Some useful utilities\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "def z_clip(xs, b):\n",
    "    return [min(x, b) for x in xs]\n",
    "\n",
    "def clip(xs, upper, lower):\n",
    "    return [max(min(x, upper), lower) for x in xs]\n",
    "\n",
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n",
    "\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def your_code_here():\n",
    "    return 1\n",
    "\n",
    "def test(msg, value, expected):\n",
    "    if value == expected:\n",
    "        print(f\"{msg}: {value}, as expected\")\n",
    "    else:\n",
    "        print(f\"{msg}: OH NO! Got {value}, but expected {expected}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')\n",
    "\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(theta, xi):\n",
    "    label = np.sign(xi @ theta)\n",
    "    return label\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(theta, X_test) == y_test)/X_test.shape[0]\n",
    "\n",
    "# Simple gradient descent algorithm\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "def gradient_descent(iterations):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77874834144183991"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (30 points)\n",
    "\n",
    "Implement a function `dp_gradient_descent` that performs differentially private gradient descent by adding noise to the gradient at each iteration. Your function should take additional arguments $\\epsilon$ and $\\delta$, and should have an **overall** privacy cost of $(\\epsilon, \\delta)$-differential privacy. You may target *either* bounded or unbounded differential privacy.\n",
    "\n",
    "**Note**: this is a major difference from the function defined in the notes, which bounds $\\epsilon$ *per-iteration*. Your solution should bound the *total* privacy cost of training.\n",
    "\n",
    "*Hint*: Use `gaussian_mech_vec`, defined above, to add noise.\n",
    "\n",
    "*Hint*: Use advanced composition to bound the total privacy cost. Start with the total privacy cost of $k$-fold adaptive composition under advanced composition, then solve for $\\epsilon_i$, the privacy cost per iteration. Use this result to set the per-iteration value of `epsilon`, and similar for `delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.01, final accuracy: 0.7335249889429456\n",
      "Epsilon = 0.1, final accuracy: 0.7389429455992923\n",
      "Epsilon = 1.0, final accuracy: 0.7674701459531181\n"
     ]
    }
   ],
   "source": [
    "def dp_gradient_descent(iterations, epsilon, delta):\n",
    "    return your_code_here()\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "for epsilon in [0.01, 0.1, 1.0]:\n",
    "    theta = dp_gradient_descent(10, epsilon, delta)\n",
    "    acc = accuracy(theta)\n",
    "    print(f\"Epsilon = {epsilon}, final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "In 2-5 sentences, argue that your implementation of `dp_gradient_descent` satisfies $(\\epsilon, \\delta)$-differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (40 points)\n",
    "\n",
    "Implement a function `zcdp_gradient_descent` that performs differentially private gradient descent by adding noise to the gradient at each iteration. Your function should take an additional argument $\\rho$, and should have an **overall** privacy cost of $\\rho$-zero concentrated differential privacy. You will also have to implement `gaussian_mech_vec_zcdp`, the vector-valued gaussian mechanism for zCDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 1e-05, final accuracy: 0.7838345864661654\n",
      "rho = 0.0001, final accuracy: 0.7754312251216275\n",
      "rho = 0.001, final accuracy: 0.7820654577620522\n"
     ]
    }
   ],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    return your_code_here()\n",
    "\n",
    "def zcdp_gradient_descent(iterations, rho):\n",
    "    return your_code_here()\n",
    "\n",
    "for rho in [0.00001, 0.0001, 0.001]:\n",
    "    theta = zcdp_gradient_descent(10, rho)\n",
    "    acc = accuracy(theta)\n",
    "    print(f\"rho = {rho}, final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Implement a function `convert_zCDP_eps_delta` that converts a $\\rho$-zCDP privacy bound to a $(\\epsilon,\\delta)$-differential privacy bound, given a target $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 1e-05, epsilon = 0.021469660262893472, final accuracy: 0.7472357363998231\n",
      "rho = 0.0001, epsilon = 0.06796140424415112, final accuracy: 0.7778637770897833\n",
      "rho = 0.001, epsilon = 0.21559660262893474, final accuracy: 0.7789694825298541\n"
     ]
    }
   ],
   "source": [
    "def convert_zCDP_eps_delta(rho, delta):\n",
    "    return your_code_here()\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "for rho in [0.00001, 0.0001, 0.001]:\n",
    "    theta = zcdp_gradient_descent(10, rho)\n",
    "    acc = accuracy(theta)\n",
    "    epsilon = convert_zCDP_eps_delta(rho, delta)\n",
    "    print(f\"rho = {rho}, epsilon = {epsilon}, final accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
